\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{comment}
\usepackage{hyperref}
%page boarders
\usepackage[vmargin=3cm, hmargin=3cm]{geometry}
\usepackage{float}
\floatstyle{plaintop}
\usepackage{paralist} %compactitem
\usepackage{fixltx2e} %some latex fixes
\usepackage{microtype} %Subliminal refinements towards typographical perfection
\setlength{\emergencystretch}{2em}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs} %Support use of the Raph Smithâ€™s Formal Script font in mathematics
\usepackage[sc]{mathpazo} %mathematical fonts
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[margin=10pt, font=small, labelfont=bf]{caption}

\usepackage{color}
\newcommand\todo[1]{\textcolor{red}{(TODO: #1)}}
\newcommand\load{L_{\mathrm{max}}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% within an align* environment, number and label the equation. #1 is the label text to be appended to eqn:
\newcommand\neqn[1]{\numberthis\label{eqn:#1}}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{invariant}{Invariant}
\begin{document}

\title{The Power of Two Random Choices\\ 
\large A follow up seminar to the Parallel Algorithms lecture\footnote{Jesper Larsson Tr\"aff and Francesco Versaci, lecture ``Parallel Algorithms'', 2012 winter term at TU Wien.}}
\author{Martin Kalany, 0825673}

\maketitle
\begin{abstract}
Balls-into-bins is a classical allocation problem, where $m$ balls have to be distributed among $n$ bins as evenly as possible. A well studied approach is to select a bin uniformly and independently at random for each ball. Another strategy is to select $d\geq 2$ bins for each ball and put the ball into the bin with the least load. Somewhat surprisingly, the maximum load achieved by such a multiple-choice scheme is significantly smaller, even for $d=2$. In this paper, we will present two different multiple-choice allocation strategies for the balls-into-bins problem and provide a detailed analysis of the upper bounds on the maximum load that they achieve. We will use a technique called witness trees, which has proven useful in a wide variety of balls-into-bins problems.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Suppose that $m$ balls are to be thrown into $n$ bins, according to some placement strategy. We are interested in the number of balls in the fullest bin, also called the \emph{maximum load}. Equivalently, one can look at the \emph{additive gap} of the allocation, i.e., the difference of the number of balls in the fullest bin and the average amount of balls in the bins. These balls-into-bins games (BiB) have been studied extensively (see, e.g., \cite{JK77}). In the context of Computer Science, the balls-into-bins model is useful to study, e.g., strategies for distributing tasks (balls) to processors (bins) as evenly as possible without having to query all the processors for the number of tasks currently assigned to them.

\subsection{Balls-into-bins model}
\label{sec:model}
Although several different variants of this balls-into-bins game exist, we will focus on a very basic model to highlight the crucial ideas in the placement strategies aand their analysis. In our model, we start with $n$ bins and put balls into them, one per time step $t$. We assume that at any time step $t$ at most $m = h n$ balls are in the bins, where $h$ is some constant greater or equal to $1$. At each time step $t$, we delete one ball from the bins. While most of the paper and the analyses focuses on the lightly loaded case where $m=n$, we will give results for the case $m > n$ as well. Throughout the paper, we will denote the maximum load by $\load$ and the additive gap by $\delta$:
\begin{align*}
\delta = \load - \frac{m}{n}.
\end{align*}

\subsection{Single-choice BiB}
\label{sec:single-choice}
A simple strategy for the balls-into-bins problem is to choose one bin uniformly and independently at random for each ball, which we will call the \emph{single-choice balls-into-bins} strategy. Using this strategy, the maximum load of the fullest bin will be
\begin{comment}
\footnote{A more accurate result exists: The maximum load is $\load = \Gamma^{-1}\left(n\right)\left(1+O\left(\frac{1}{\log \Gamma^{-1}\left(n\right)}\right)\right)$ \cite{G91}.}
\end{comment}
\begin{align*}
\load = \frac{\ln n}{\ln \ln n} \cdot \left(1 + o\left(1\right)\right)
\end{align*}
for the case $m=n$, with high probability\footnote{We say that an event $\mathcal E$ occurs \emph{with high probability} (w.h.p.) if $\Pr\left[\mathcal E \right] \geq 1 - n^{-\alpha}$ for an arbitrary constant $\alpha > 0$.}~\cite{RS98}. This implies\footnote{For a detailed proof, see Theorem \ref{theorem:asymptotic} in Appendix \ref{sec:asymptotic}.}
\begin{align*}
\load\in \Theta\left(\frac{\ln n}{\ln \ln n}\right).
\end{align*}
Note that since the average load is 1, $\delta = L_{max}$ in this case. For the heavily loaded case, w.h.p.~the maximum gap is at most~\cite{RS98}.
\begin{align*}
\delta = \Theta\left(\sqrt{\frac{m \cdot \ln n}{n}}\right).
\end{align*}

\subsection{Multiple-choice BiB}
Consider a variation of the above \emph{single-choice balls-into-bins} strategy: For each ball, chose $d \geq 2$ bins independently and uniformly at random and place the ball into the \emph{least full bin}\footnote{If there are multiple bins with the same minimal load, chose one among them arbitrarily.}. This simple \emph{multiple-choice balls-into-bins} scheme, which we will call the \emph{Greedy} algorithm\footnote{A detailed definition will follow in Section \ref{sec:greedy}}, achieves a gap
\begin{align*}
\delta = \frac{\log \log n}{\log d} + O(1)
\end{align*}
 w.h.p.~for all $m$~\cite{ABKU99}~\cite{BCSV06}.
 
This apparently minor change introduces a small amount of choice which, compared to the single-choice scheme, leads to a significant reduction in the maximum load. 
\begin{comment}
Curiously, the exponential decrease in the gap is already achieved having just two choices (that is, $d=2$). Each additional choice decreases the bound for the maximum gap by just a constant factor~\cite{MRS01}. 
\end{comment}
Extensive research following the influential paper by Azar et al.~\cite{ABKU99}, who were the first to explicitly prove this phenomenon\footnote{For a brief history of the proof, which is quite interesting by itself, refer to Appendix \ref{sec:historyOfGreedy}.}, suggests that this \emph{two-choice paradigm} is not just an artifact of the simple balls-into-bins model, but a general principle applicable to a variety of problems. 

Consider a slightly more involved strategy, which we will call the \emph{Always-go-left} scheme: Suppose that the $n$ bins are partitioned evenly into $d$ groups, each of size\footnote{For the sake of a clear presentation, we will assume that $d$ divides $n$ evenly in the remainder of this paper.} $\lfloor \frac{n}{d}\rfloor$ or $\left\lceil\frac{n}{d}\right\rceil$. We select one bin from each group uniformly and independently at random and put the ball into the least full of those $d$ bins. In case a tie occurs, put the ball into the \emph{left-most} bin. That is, assuming a fixed ordering of bins $b_i$ for $1 \leq i \leq n$, from among the $d' \leq d$ bins with the same minimal load, we put the ball into the bin with smallest index $i$. Somewhat surprisingly, V\"ocking \cite{VOC03} showed that such a strategy employing \emph{asymmetric tie-breaking} in combination with \emph{non-uniform} bin selection reduces the bound for the gap to\footnote{The symbol $\Phi_d$ denotes a generalization of the golden ratio, based on generalized Fibonacci numbers. Refer to Appendix \ref{sec:fibonacci} for the definition.}
\begin{align*}
\delta = \frac{\ln\ln n}{d \cdot \ln \Phi_d} + O(1)
\end{align*}
w.h.p. Not only is this a significant reduction of the asymptotic bound, V\"ocking was able to show that the Always-go-left strategy is optimal up to additive constants \cite{VOC03}.

This paper is organized as follows: We give a basic classification of balls-into-bins games in Section \ref{sec:classification}, to provide some overview of the different directions in research as well as possibilities of abstraction from applications, some of which are listed in Section \ref{sec:applications}. In the remaining sections of this paper we will consider the model given in Section \ref{sec:model}, which can be classified as a \emph{sequential, finite and dynamic process with random deletions}. Section \ref{sec:multiple-choice} starts with a brief definition of three different classes of random placement strategies and then continues with the presentation of the two placement algorithms outlines in the introduction: The  \emph{Greedy} algorithm (Section \ref{sec:greedy}) and the \emph{Always-go-left} algorithm (Section \ref{sec:AlwaysGoLeft}). Section \ref{sec:analysis} is the core of this paper and will give a detailed analysis of both strategies. We will use a proof technique called \emph{witness trees}, which allows for a clean analysis of both problems. Note that the technique is applicable to a wide variety of balls-into-bins scenarios, making it the most useful of several proof techniques applied to balls-into-bins problems. Other proof techniques such as \emph{layered induction} and a \emph{differential equations approach} have been used. They are however not part of this paper. See, e.g., \cite{MRS01} for an overview of different proof techniques for the balls-into-bins problem.

\begin{comment}
In Section \ref{sec:strategies}, we will provide asymptotic bounds for the ad dative gap of this strategy, which we will then use to devise more complex strategies that significantly reduce these bounds. 
\end{comment}

\begin{comment}
These result may sometimes seem surprising, which is why we will denote a significant part of this article to demonstrate proofs in detail. This proof technique may be useful for devising asymptotic bounds for the additive gap of different balls-into-bins scenarios and strategies.
\end{comment}


\section{Classification}
\label{sec:classification}
Several different variations of this problem exist, which may be classified by the following properties:
\begin{compactitem}
\item In a \emph{sequential process}, the balls arrive one after another. A  ball is dealt with only after the previous ball has been placed. Contrary, in a \emph{parallel process}, balls arrive in sets and the balls in one set are dealt with simultaneously \cite{ABS98}. 
\item In a \emph{static problem}, the balls are placed into the bins and no balls are ever removed. In a \emph{dynamic problem}, balls can also be removed from the system. In a simple \emph{random deletions} model, for each newly inserted ball a ball chosen uniformly and independently at random from among the balls currently in the bins is removed from the system \cite{ABKU99} \cite{MRS01}. More elaborate schemes exists: An \emph{oblivious adversary} specifies the sequence of insertions and deletions of balls in advance, without any knowledge of the placement of the balls \cite{CFM+98}. Another approach was studied by Adler et al.~\cite{ABS98} in the context of the parallel balls-into-bins problem: In each bin, the balls are stored in FIFO-order and at every time step the first ball in each of the bins is deleted. 
\item A \emph{finite process} is analyzed for a given time interval that is known beforehand, whereas for an \emph{infinite process}, a placement and a deletion process is defined over an infinite time span \cite{ABS98}.
\item In the \emph{lightly loaded case}, $m$ is in the order of $n$, whereas in the \emph{heavily loaded case}, $m$ is arbitrarily large. For clarity, we will assume that $m = n$ in the lightly loaded case and that $m > n$ in the heavily loaded case. 
\end{compactitem}

\section{Applications}
\label{sec:applications}
The two-choice paradigm applied to the balls-into-bins model has several useful applications, some of which we will outline here. The applications in \emph{Hashing} (Section \ref{sec:hashing}), \emph{Online Load Balancing} (Section \ref{sec:loadbalancing}) and \emph{Emulation of shared memory machines on distributed memory machines} \ref{sec:DMM}) share a close relation to load balancing. Section \ref{sec:circuitrouting} applies the balls-into-bins model and the two-choice paradigm in a different context, namely that of routing circuits in multi-stage interconnection networks with minimal congestion.

\subsection{Hashing}
\label{sec:hashing}
A simple hash table implementation typically uses a single hash function to map elements to entries in a table. If more than one element is mapped to the same entry, all elements mapped to this table entry are stored in a \emph{chain}, usually implemented as a linked list. The maximum time to search for an element is then proportional to the length of the longest chain in the hash table. 

It is easy to see how the balls-into-bins idea can be applied: Assuming a perfect hash function\footnote{i.e., each of the $n$ elements is mapped to one of the $n$ table entries uniformly and independently at random}, the elements are balls to be mapped to table entries or bins, implying a worst case look-up time of $\Theta\left(\frac{\log n}{\log \log n}\right)$ w.h.p. Applying the two-choice paradigm by using two hash functions and mapping elements to the least full of two table entries implies a bound of $\Theta\left(\log \log n\right)$ w.h.p. This approach is easy to parallelize, it does not require re-hashing of data and requires only two hash functions \cite{ABKU99} \cite{MRS01}.

\subsection{Online load balancing}
\label{sec:loadbalancing}
The following simple scenario is a typical application of online load balancing: Consider $n$ servers\footnote{Such as file-, database- or network servers} and $m$ requests issued by clients that need to be handled by these servers. For simplicity, assume that each task can be handled by any server and that all tasks are of the same size, i.e., require the same amount of execution time.

To maximize the throughput of the servers, a uniform distribution of tasks to servers is desirable. This can easily be achieved by a central dispatcher. However, such a central dispatcher poses a bottleneck within a distributed system and querying $m$ servers for their current load involves significant overhead, e.g., sending a message to each server and waiting for the reply and more efficient solutions are necessary. As with hashing, the balls-into-bins model can be applied easily: we need to assign tasks (the balls) to servers (the bins). Assuming tasks are assigned to servers uniformly and independently at random, the two-choice paradigm is directly applicable: If each client samples two random servers for their load and sends issues its request to the least loaded server, the overhead is comparatively small and the difference in load on the $n$ servers is bounded by $\Theta\left(\log \log n\right)$ w.h.p. \cite{KLM92} \cite{RS98} \cite{MRS01}.  

\subsection{Emulation of shared memory machines on distributed memory machines}
\label{sec:DMM}
The \emph{Parallel Random Access Machine} model\footnote{as defined in the lecture or in~\cite{P03}}, or \emph{PRAM} for short, is a high level abstraction of a shared memory machine, in which memory access by any processor to any memory location is assumed to require constant time. It is especially useful for the study of parallel algorithms. However, a PRAM is much harder to realize in hardware than a \emph{Distributed Memory Machine} (DMM). Thus, ways to emulate a PRAM on a DMM have been studied extensively. Such an emulation distributes the processors and memory cells of the PRAM to the processors and memory modules (typically one per processor) of the DMM with the goal to reduce the slowdown of the emulation, which is the time the DMM requires to simulate one step of the PRAM. Each memory module can handle only one memory access at a time step. More accesses to the same module will result in memory contention. The balls-into-bins idea is useful for managing communication between the processors and the memory modules such that memory contention is minimal. For details, see Karp et al.~\cite{KLM92} and the papers referenced there. 
 
\subsection{Low congestion circuit routing}
\label{sec:circuitrouting}
In multi-stage interconnection networks, virtual circuit-switching is used to route communication. For simplicity, we consider the \emph{permutation routing problem}, where one request originates at each of the $n$ input nodes and one request is destined for each of the $n$ output nodes and assume a \emph{butterfly network} as the underlying interconnection network (see \cite{CLR09} for a detailed definition). Then, congestion is the maximum number of paths that are routed through a network link. The goal of a \emph{circuit routing algorithm} is to to allocate paths for all $n$ communication requests such that congestion is minimal.

In this context, \emph{Valiant's paradigm} \cite{V82} states that any such permutation routing problem can be solved by transforming it into two problems: First, find a path for each request to an intermediate destination chosen uniformly and independently at random. Next, find a path from the intermediate to the actual destination. This routing technique is analogous to the single-choice balls-into-bins problem where each communication request (or ball) chooses an intermediate destination (a bin) at random. In this analogy, the congestion achieved by Valiant's paradigm corresponds to the maximum load in the balls-into-bins model, thus implying a bound of $\Theta\left(\log \log n\right)$ on the congestion w.h.p. \cite{CMM+98} \cite{MRS01}. 

Applying the two-choice paradigm in this context is somewhat more involved and beyond the scope of this paper. The interested reader may find more details in \cite{CMM+98}.

\section{Multiple-choice balls-into-bins}
\label{sec:multiple-choice}
The \emph{single-choice balls-into-bins strategy} (Section \ref{sec:single-choice}) chooses one bin uniformly and independently at random for each ball. A \emph{multiple-choice balls-into-bins strategy} generalizes this idea by  choosing $d \geq 2$ bins and placing the ball into the least full bin. Depending on how the $d$ locations are sampled, we distinguish three classes of placement algorithms \cite{VOC03}. 

\section{Classes of random placement strategies}
\label{sec:classesOfPlacement}
Let $B$ be the set of bins $\{1,2...n\}$, $\Omega = B^{d}$ be the sample space and $\mathcal{F} = 2^{\Omega}$ be the set of measurable events. We are only interested in events that represent a set of bins of size $d$. Thus, let $\mathcal{F}_d = \{\mathcal{E} \in \mathcal{F}: \left\vert \mathcal{E} \right\vert = d\}$ and set $\Pr\left(\mathcal{E}  \right) = 0$  $\forall \mathcal{E} \in \mathcal{F} \setminus \mathcal{F}_d $. The three classes of algorithms are distinguished by the probability function $\Pr\left(\mathcal{E}\right)$ for events $\mathcal{E} \in \mathcal{F}_d$:
\begin{compactitem}
\item  \emph{Uniform and independent.} Choose each of the $d$ bins for a ball uniformly and independently at random from the set of bins $B$. This implies 
\begin{align*}
\forall \mathcal{E} \in \mathcal{F}_d: \Pr\left(\mathcal{E}\right) = \frac{1}{n ^{d}}
\end{align*}
\item Class 2: \emph{Non-uniform and independent.} Choose the i-th bin from $B$ independently at random according to some probability function
\begin{align*}
\Pr: B \rightarrow \left[0,1\right]
\end{align*}

\item Class 3: \emph{Non-uniform and dependent.} Choose the $d$ bins for a ball from $\Omega$ at random:
\begin{align*}
\Pr: B^{d} \rightarrow \left[0,1\right]
\end{align*}
\end{compactitem} 
 
We will discuss two different multiple-choice placement strategies: The \emph{Greedy} algorithm (Section \ref{sec:greedy}) belongs to class 1, whereas the \emph{Always-go-left} scheme (Section \ref{sec:AlwaysGoLeft}) uses a non-uniform and independent selection of $d$ bins and thus belongs to class 2.

\subsection{The Greedy scheme}
\label{sec:greedy}

Consider a variation of the above \emph{one-choice balls-into-bins} strategy, which we will call the \emph{Greedy} scheme: For each ball, choose $d \geq 2$ bins independently and uniformly at random and place the ball into the \emph{least full bin}. If there are multiple bins with the same minimal load, chose one among them arbitrarily. Note that the $d$ selected bins are not necessarily distinct. 

\begin{theorem}
\label{theorem:greedy}
Consider any sequence of insertions and deletions s.t.~at most $hn$ balls are in the bins at any time. If the balls are placed into bins by the Greedy algorithm, the maximum load is at most 
\begin{align*}
\frac{m}{n} + \frac{\log \log n}{\log d}+ O(h)
\end{align*}
w.h.p.~at any time $t$ \cite{ABKU99} \cite{BCSV06}.
\end{theorem}

This implies a gap of $\Theta\left(\log \log n \right)$ for arbitrarily large $m$, which is remarkable in several ways:
\begin{compactitem}
\item The apparently small change made for the \emph{Greedy} algorithm compared to the \emph{single-choice balls-into-bins} scheme results in an exponential decrease of the gap, even for $d=2$. 
\item Each additional choice ($d > 2$) decreases the gap by only a constant factor \cite{MRS01}. 
\item For the \emph{heavily loaded case}, the resulting gap of the \emph{multiple-choice balls-into-bins} strategy does not depend on the number of balls $m$. In contrast, the bound for the \emph{one-choice balls-into-bins} scheme diverges with $m$.
\begin{comment}
\todo{the following is only true for class 1 algorithms}
\item The given bounds are \emph{tight}, meaning that no other strategy that places each ball into one of $d$ randomly selected bins achieves a gap that is asymptotically lower.
\end{comment}
\end{compactitem}

We will provide a detailed proof of this property in Section \ref{sec:analysis}.

\subsection{The Always-go-left scheme}
\label{sec:AlwaysGoLeft}
The \emph{Greedy} strategy presented in Section \ref{sec:multiple-choice} assumes that $d$ bins are chosen uniformly and independently at random. V\"ocking \cite{VOC03} was the first to study the impact of non-uniform and dependent selections of the $d$ bins. Surprisingly, a multiple-choice balls-into-bins strategy with a \emph{non-uniform} and independent selection of the $d$ bins, such as the \emph{Always-go-left} algorithm introduced in \cite{VOC03}, achieves better \cite{BCSV06} load balancing than the \emph{Greedy} scheme (Section \ref{sec:greedy}). Furthermore, V\"ocking proved that the \emph{Always-go-left} algorithm is optimal up to additive constants.

The \emph{Always-go-left} algorithm works as follows. Partition the $n$ bins into $d$ groups, each of size $\left\lfloor \frac{n}{d}\right\rfloor$ or $\left\lceil\frac{n}{d}\right\rceil$ . We select one bin from each group uniformly and independently at random and put the ball into the least full of those $d$ bins. In case a tie occurs, put the ball into the \emph{left-most} bin. That is, assuming a fixed ordering of bins $b_i$ for $1 \leq i \leq n$, from among the $d' \leq d$ bins with the same minimal load, put the ball into the bin with smallest index $i$.

\begin{theorem}
\label{theorem:agln}
If $n$ balls are placed into $n$ bins using the Always-go-left scheme, the number of balls in the fullest bin will be 
\begin{align*}
\frac{\ln\ln n}{d \cdot \ln \Phi_d} + O(1)
\end{align*}
w.h.p.
\end{theorem}

\begin{theorem}
\label{theorem:algm}
Consider any sequence of insertions and deletions s.t.~at most $hn$ balls are in the bins at any time. If the balls are placed into bins by the Greedy algorithm, at any time $t$ the maximum load is at most 
\begin{align*}
\frac{\ln\ln n}{d \cdot \ln \Phi_d} + O(h)
\end{align*}
w.h.p.
\end{theorem}

\section{Analysis}
\label{sec:analysis}
In this section, we will prove Theorems \ref{theorem:greedy}, \ref{theorem:agln} and \ref{theorem:algm} in detail, using \emph{witness trees} to provide an upper bound for the event that a bin contains too many balls. Our analysis closely follows the proofs in \cite{VOC03}. \todo{is it enough to cite this once for the whole section?}

We say that a \emph{bad event} occurs when the maximum load exceeds some given threshold value. When such a bad event occurs we construct an \emph{activated witness tree}. In other words, the activation of a witness tree is implied by a bad event. The probability for the existence of such an activated witness tree is an upper-bound for the probability that the associated bad event occurs.

Throughout this section, \emph{location} will denote the a specific place in one of the $n$ bins. A location of a ball $b$ refers not only to a bin, but also to the number of balls already in this bin at the time of insertion of ball $b$.

We will prove the bounds for $m=n$ for both the Greedy and the Always-go-left scheme in Sections \ref{sec:analysis:greedy} and \ref{sec:analysis:alg}, respectively. However, we use a simplifying Assumption \ref{assumption:independence} in those two sections to provide a more clear presentation of the proof technique. Section \ref{sec:analysis:nondistinctBalls} will extend these proofs to remove the simplifying assumption we make initially. The witness tree method can be used to show that the same bounds hold for the heavily loaded case where $m> n$ too, but we do not give such a proof in this paper. Instead, we refer the interested reader to V\"ocking \cite{VOC03}. Berenbrink et al.  ~\cite{BCSV06}, Talwar et al.~\cite{TW13} and Mitzenmacher \cite{MRS01} provide proofs the heavily loaded case using different approaches as well.

\subsection{Greedy}
\label{sec:analysis:greedy}

\subsubsection{Symmetric witness trees}
\label{sec:analysis:definitionSymWT}
A \emph{symmetric witness tree} of order $L$ is a complete $d$-ary tree with $d^{L}$ leaves. Each node $v$ represents a ball $b_v = \mathrm{ball}(v)$ . The nodes and edges of the witness tree represent events that may occur or not. The same ball may be represented by several nodes. However, initially we will make use of the following assumption in the analysis.
\begin{assumption}
\label{assumption:independence}
All events represented by the nodes and edges of a witness tree are stochastically independent. 
\end{assumption}
In other words, we assume that no ball is referenced by more than one node of a witness tree. Obviously, a correct analysis will have to consider balls assigned to more than one node, but this simplifying assumption is useful to highlight the differences between the symmetric and asymmetric allocation schemes. Section \ref{sec:analysis:nondistinctBalls} will extend the analysis to witness trees with redundant balls.

We define edge and leaf events:
\begin{compactitem}
\item \emph{Edge event:} Consider some edge $e = (u,v)$ of the witness tree, s.t.~$v$ is the i-th child of $u$. Then $e$ represents the event that the i-th location of the ball $b_u = \mathrm{ball}(u)$ points to the same bin as one of the locations of the ball $b_v = \mathrm{ball}(v)$. 
%$\todo{illustrate}
\item \emph{Leaf event:} A leaf node $v$ of the witness tree represents the event that each of the $d$ locations of the ball $b_v = \mathrm{ball}(v)$ points to a bin that contains at least three additional balls (i.e., balls that are not represented by a node of the tree) at the time of the insertion of the ball $b_v$.
%\todo{illustrate}
\end{compactitem}
The definition for leaf events refers to bins containing some number of balls at some specified time. To avoid dependencies, the crucial trick is that edge events are defined in terms of the alternative locations of balls, not by their finally assigned bin. 

We say that an edge or leaf event is \emph{activated} if the associated event occurs. A witness tree is activated when all its edge and leaf events are activated. 

The following constraints have to be fulfilled by an assignment. The ball $b_r$ that is represented by the root node $r$ has to exist at time step\footnote{As defined in Theorem \ref{theorem:greedy}} $t$ and each ball $b_v$ represented by a node $v \neq r$ and parent node $u$ has to exist when the ball $b_u = \mathrm{ball}(u)$ is inserted.

Assume the bad event that some bin contains more than $L+3$ balls at time $t$ occurs. We will use an activated symmetric witness tree of order $L$ to witness this event and provide an upper-bound for the occurrence of the event. Thus, we have to show that the existence of a bin $x$ containing at least $L+4$ balls implies the existence of an activated witness tree of order $L$. 

\begin{lemma}
When using the Greedy algorithm to put balls into bins and arbitrary $L > 0$, a bin containing more than $L+3$ balls implies the activation of a symmetric witness tree of order $L$.
\end{lemma}

\begin{proof}
We construct a witness tree for the event that some bin contains $L+4$ balls as follows:
\begin{compactenum}
\item The topmost ball $b_1$ in bin $x$ is assigned to the root node $v_r$. Note that $b_1$ is the last ball that was inserted into the bin $x$.
\item Since the Greedy-scheme puts ball $b_1$ into bin $x$ only if each of the $d$ sampled locations contain at least the same number of balls at bin $x$, we can conclude that each of the $d$ locations contain at least $L+3$ balls at the time ball $b_1$ was inserted. Thus, the $d$ children of the root node are assigned the topmost balls of these $d$ bins.
\item Consider these child nodes and continue the assignment of the previous step recursively until the leaf nodes are reached.  
\end{compactenum}

By construction, all edge events of the witness tree are activated. Note that the ball assigned to the root node is on top of at least $L+3$ balls. The balls assigned to the root's child node are each on top of $L+2$ or more balls and so on. It follows that each ball assigned to a leaf node is on top of at least 3 other balls, implying that the $d$ locations of each leaf node point to bins with at least 3 balls in each. This implies the activation of all leaf nodes of the witness tree.
\end{proof}

\subsubsection{Probability of activation}
\label{sec:analysis:probabilitySymWT}
In this section, we derive the probability that some witness tree of order $L$ is activated, which gives an upper bound for the probability that the maximum load is $L+3$ at some time $t$. 

\begin{lemma}\label{lemma:swt:activation}
For some arbitrary $\alpha$ and large enough $L$, the activation of a symmetric witness tree of order $L$ occurs with probability at most $n^{-\alpha}$.
\end{lemma}
Speaking less formaly, Lemma \ref{lemma:swt:activation} states that the for large enough $L$, the activation of a symmetric witness tree is unlikely.

\begin{proof}
The probability $p_{swt}$ that some symmetric witness tree of order $L$ is activated is bounded by the number of witness trees $w$ times the upper bound on the probability $p_1$ that a specific witness tree is activated.

The number of different witness trees $w$ is given by the number of ways to assign balls to its nodes. The ball assigned to the root node can be chosen from at most $n$ balls, since by definition (Theorem \ref{theorem:greedy}), at most $n$ balls are in the bins at any time $t$ and a ball assigned to a node has to be in some bin at time $t$. Each child node of the root is assigned one out of at most $n$ balls too by the same argument. Applying this argument for each level of the witness tree, there are at most $w = n^k$ possibilities to assign balls to a witness tree, with $k = d^{L+1}-1$ being the number of nodes in a $d$-ary witness tree of order $L$.
 
An edge\footnote{where $v$ is the i-th child of $u$} $(u, v)$ is activated with probability at most $d/n$, which is the probability that the i-th location of the ball $b_u = \mathrm{ball}(u)$ points to a particular location of the ball $b_v = \mathrm{ball}(v)$ times the number of locations $d$. This implies a probability of at most
\begin{align*}
p_e = \left(\frac{d}{n}\right)^{k-1}
\end{align*} 
or the event that all $k-1$ edges of a witness tree are activated. Note that  the assumption that all balls are distinct (Assumption \ref{assumption:independence}) implies that all edge events are independent.

The probability that a leaf event is activated is bounded above by $3^{-d}$. Recall that by the definition of leaf events, each of the $d$ locations of the ball $b_{vl} = \mathrm{ball}(v_l)$ has to point to a bin with at least 3 additional balls in it. The number of those bins is bounded by $\lfloor n/3 \rfloor$ at all time steps $t$, implying a probability of at most 
\begin{align*}
\frac{n/3}{n} = \frac{1}{3}
\end{align*}
that one such bin is chosen. This implies a probability of at most $3^{-d}$ that all $d$ bins are chosen among those with at least 3 additional bins in them. Thus, the probability that all leaf events are activated is at most $p_l = 3^{-d\cdot l}$, where $l = d^{L}$ is the number of leaves in a witness tree.

Thus, the probability that an activated symmetric witness tree exists at time $t$ is at most 
\begin{align*}
p_{swt} &= w \cdot p_e \cdot p_l \\
		&= n^{k} \cdot \left(\frac{d}{n}\right)^{k-1} \cdot 3^{-d \cdot l} \\
		&= n \cdot d^{k-1} \cdot 3^{-d \cdot l}
\end{align*}
Using $k \leq 2l$ we get
\begin{align*}
p_{swt} &\leq n \cdot d^{2\cdot l} \cdot 3^{-d \cdot l} \\
		&= n \cdot \left(d^2\right)^l \cdot \left( 3^{-d} \right)^l
\end{align*}
which we simplify by using $2d^2 \leq 3^d$
\begin{align*}
p_{swt} &\leq n \cdot \left(\frac{3^d}{2}\right)^l \cdot \left( 3^{-d}\right)^l \\
		&= n \cdot 2^{-l} \cdot 3^{d\cdot l} \cdot 3^{-d \cdot l} \\
		&= n\cdot 2^{-l}		\\
		&= n\cdot 2^{-d^L}		
\end{align*}

For any constant $\alpha > 0 $, let
\begin{align*}
L &\geq \log_d\log_2 n + \log_d\left(1+\alpha\right) \neqn{swt:chooseL}
\end{align*}

Then, 
\begin{align*}
p_{swt} &\leq n \cdot 2^{{-d}^{\log_d\log_2 n + \log_d\left(1+\alpha\right)}} \\
		&= n \cdot 2^{-{d}^{\log_d\log_2 n}\cdot d^{\log_d \cdot\left(1+\alpha\right)}} \\
		&= n \cdot \left( 2^{-\log_2 n \cdot \left(1+ \alpha \right)} \right) \\
		&= n \cdot n^{-\left(1+\alpha\right)} = n^{-\alpha} \neqn{swt:result}
\end{align*}
Thus, for any constant $\alpha$ and large enough $L$ as specified in Equation \ref{eqn:swt:chooseL} the probability that a symmetric witness tree of order $L$ is activated is bounded above by $p_{swt }\leq n^{-\alpha}$.
\end{proof}

Since the probabilty for the activation of a symmetric witness tree of order $L$ upper-bounds the probability of the bad event that some bin contains more than $L+3$ balls, Equation \ref{eqn:swt:result} also is an upper bound for the latter. Note that Equation \ref{eqn:swt:chooseL} can be written as
\begin{align*}
L &\geq  \frac{\ln\log_2 n}{\ln d} + \frac{\ln(1+\alpha)}{\ln d} \\
   &= \frac{\ln \log_2 n}{\ln d} \cdot \left(1 + \frac{\ln\left(1+\alpha\right)}{\ln \log_2 n}\right) \\
  &=  \frac{\ln \ln n}{\ln d} \cdot \left( 1+ o\left( 1\right)\right) \neqn{swt:boundL}
\end{align*}
This proves Theorem \ref{theorem:greedy} for the case $m=n$, under the assumption that all events are stochastically independent (Assumption \ref{assumption:independence}). \qed

\subsection{Always-go-left}
\label{sec:analysis:alg}
Similarly to Section \ref{sec:analysis:greedy}, where we used symmetric witness trees to upper-bound the probability that a bin contains too many balls when using the \emph{Greedy} scheme, we will use asymmetric witness trees to get an analogous upper bound for the \emph{Always-go-left} scheme. We will make the same simplification as stated in Assumption \ref{assumption:independence}, namely that all the events represented by the edges and nodes of a witness tree are stochastically independent.

\subsubsection{Asymmetric witness trees}
\label{sec:analysis:definitionAsymWT}
With one exception, the definition of an \emph{asymmetric witness tree} is the same as the definition of a symmetric witness tree, including the definition of activation. Only its structure differs from that of a symmetric tree. An asymmetric witness tree has the structure of a $d$-ary Fibonacci tree $T_d\left(k \right)$, which is defined as follows.
\begin{compactitem}
\item $T_d(1)$ is a single node.
\item $T_d(2) = T_d(1)$.
\item For $3\leq k \leq d$, $T_d(k)$ is a tree with $k-1$ children, which are the roots of the trees $T_d(k-1),\ldots,T_d(1)$.
\item For $k>d$, $T_d(k)$ is a tree with $d$ children, which are the roots of the trees $T_d(k-1),\ldots,T_d(k-d)$.
\end{compactitem}
%\todo{illustrate and note difference to usual Fibonacci trees}

A $d$-ary asymmetric witness tree of order $L$ has the structure of the Fibonacci tree $T_d(d \cdot L+1)$ and by Equation \ref{eqn:goldenFib} has $F_d(d\cdot L + 1) \geq \Phi_d^{d\cdot L-1}$ leaves. 

\begin{lemma}
When using the Always-go-left algorithm to put balls into bins and arbitrary $L > 0$, a bin containing more than $L+3$ balls implies the activation of an asymmetric witness tree of order $L$.
\end{lemma}
\begin{proof}
For each node we define a label $(h, i)$, where $h, i \in \mathbb{N}$, $0 \leq h \leq L$ and  $1\leq i \leq d$. 
\begin{compactitem}
\item A node that is the root of a Fibonacci tree $T_d(1)$ is assigned the label $(0,1)$.
\item Analogously, a node that is the root of a Fibonacci tree $T_d(2)$ is assigned the label $(0,2)$. Thus, nodes labeled $(0,1)$ or $(0,2)$ do not have children.
\item A node with label $(0, i)$, where $i>2$, has $i-1$ children with labels $(0,i-1)\dots (0,1)$
\item A node with label $(h,i)$ , where $h>0$, has $d$ children with the labels $(h,i-1)\dots(h,1)$,$(h-1,d)\dots(h-1, i)$.
\item The root of a Fibonacci tree $T_d(d\cdot h+i)$ has the label $(h, i)$. 
\end{compactitem}
Thus, our the root node of our witness tree will be labeled $(L, 1)$. Note that the labels are unique within the set of children of a node $v$, but not for all nodes of the tree. The labels are useful to state the following invariant, which will be maintained while assigning balls to the nodes of the witness tree.

\begin{invariant}
Ball $b_v$ referenced by a node $v$ with label $(h,i)$ was placed in a bin belonging to group $i$. The bin contained at least $h+3$ other balls at the time of insertion of the ball $b_v$.
\end{invariant}

Let $x$ be the bin that contains $L+4$ balls. The root node gets assigned the topmost ball in bin $x$. Each of the $d$ locations of this ball points to a bin with at least $L+3$ balls in it, at the time of its insertion. These are the $d$ bins that were sampled by the Always-go-left scheme and thus bin $i$ belongs to the $i$'th group of bins, where each group consists of $n/d$ bins\footnote{For the sake of simplicity, we assume that $d$ divides $n$ evenly, i.e., that all groups have the exact same size. It can be shown that the analysis still holds if each group is of size $\lfloor\frac{n}{d}\rfloor$ or $\left\lceil\frac{n}{d}\right\rceil$.}. The child node of the root with label $(L-1, i)$ is assigned the topmost ball of the bin the $i$' location points to. The assignment of balls to nodes than proceeds recursively. Given a node $v$ with label $(h, i)$ whose ball $b_v$ was put into some bin belonging to group $i$ and containing at least $l+3$ other balls, we can conclude the following.
\begin{compactitem}
\item For $1\leq j < i$, the $j$'th location of the ball $b_v$ points to a bin containing at least $l+4$ balls at the insertion time of the ball $b_v$. If the j-th location contained less balls than the i-th location, the Always-go-left scheme would have put the ball $b_v$ into the bin pointed to by location $j$. The topmost ball in location $j$ is assigned to the  child $u$ of $v$ with label $(l,j)$. 
\item For $i < j \leq d$, the j-th location contains at least $l+3$ balls. If location $j$ contained less, the Always-go-left scheme would have put the ball $b_v$ into the bin it points to. The topmost ball in the bin that location $j$ points to is assigned to the child node $u$ of $v$ labeled with $(l-1, j)$. \todo{the paper claims that the locations $j$ point to bins with $l+2$ balls. This can not be true, since the topmost, i.e., the $l+2$ ball - call it $b_t$ - gets assigned to a node with label $(l'=l-1, j)$. Since there are $l+1=l'+2$ balls below it, at the time of the placement of ball $b_t$, the bin $j$ it gets put into has only $l'+2$ balls in it, violating the invariant. Same with the case $j<i$.} 
\end{compactitem}
\end{proof}

\subsubsection{Probability of activation}
\label{sec:analysis:probabilityAsymWT}
\begin{lemma}\label{lemma:awt:activation}
For some arbitrary $\alpha$ and large enough $L$, the activation of an asymmetric witness tree of order $L$ occurs with probability at most $n^{-\alpha}$.
\end{lemma}

\begin{proof}
Analogously to the symmetric witness tree, the possible number of asymmetric witness trees is upper-bounded by $w=n^k$ and the probability that all edge events occur is\footnote{As in Section \ref{sec:analysis:greedy}, $k$ denotes the number of nodes of the witness tree and $l$ the number of leaves.} 
\begin{align*}
p_e = \left(\frac{d}{n}\right)^{k-1}
\end{align*}

The probability that all leaf events occur too is the same as for the symmetric witness tree, but its derivation differs. Again, at any time at most $b'=n/3$ bins contain 3 or more balls, but we do not know how those bins are distributed among the groups. Let $b_i$ be the number of bins in group $i$ that contain 3 or more balls. Then $\beta_i = b_i\cdot d/ n$ is the fraction of the bins $b_i$ in group $i$. Since no more than $n/3$ bins can contain 3 or more balls, the values $b_i$ are subject to the constraint
\begin{align*}
\frac{n}{d}\sum_{i=1}^d \beta_i \leq \frac{n}{3}
\end{align*}
and thus 
\begin{align*}
\sum_{i=1}^d \beta_i \leq \frac{d}{3}
\end{align*}

The probability that all $d$ locations of a ball point to a bin containing 3 or more balls is then 
\begin{align*}
p_{l_1} = \prod_{i=1}^{d} \beta_i
\end{align*}
In other words, a leaf event occurs with probability $p_{l_1}$, which is maximized when all $b_i$ are equal. We thus set $b_i \leq 1/3$ for all $i$ and get as upper bound for the probability that all leaf events occur 
\begin{align*}
p_l \leq 3^{-d\cdot l}
\end{align*}

The above results imply the same probability for the activation of an asymmetric witness tree as we derived in Section \ref{sec:analysis:probabilitySymWT} for the activation of a symmetric witness tree, that is
\begin{align*}
p_{awt} &\leq n \cdot 2^{-l}
\end{align*}
However, the number of leaves is larger, since 
\begin{align*}
l \geq \Phi_d^{d\cdot L -1}
\end{align*}
holds\footnote{see Section \ref{sec:analysis:definitionAsymWT} for details}.

For any constant $\alpha > 0$, let 
\begin{align*}
L &\geq \left\lceil{\frac{\ln\log_2 n + \ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d}}\right\rceil+\frac{1}{d} \neqn{awt:chooseL}
\end{align*}
\todo{the paper uses $+1$, instead of $+1/d$, but I can't workout the below result then} 
Then, 
\begin{align*}
p_{awt} &\leq n \cdot 2^{-\Phi_d^{d\cdot L -1}} \\
        &= n \cdot 2^{-\Phi_d^{d\cdot \left(\left\lceil{\frac{\ln\log_2 n + \ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d}}\right\rceil+\frac{1}{d}\right) -1}} \\
        &\leq n \cdot 2^{-\Phi_d^{d\cdot \left({\frac{\ln\log_2 n + \ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d}}+\frac{1}{d}\right) -1}} \\
        &= n \cdot 2^{-\Phi_d^{{\frac{\ln\log_2 n + \ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d}}}} \\
        &= n \cdot 2^{-\Phi_d^{\log_{\Phi_d}\log_2 n + \log_{\Phi_d}\left(1+\alpha\right)}} \\
         &= n \cdot 2^{-\Phi_d^{\log_{\Phi_d}\log_2 n}\cdot \Phi_d^{ \log_{\Phi_d}\left(1+\alpha\right)}} \\
         &= n \cdot 2^{-\log_2 n \cdot\left(1+\alpha\right)} \\
         &= n \cdot n ^{-(1+\alpha)} = n^{-\alpha} \neqn{awt:result}
\end{align*}
Thus, for any constant $\alpha$ and large enough $L$ as specified in Equation \ref{eqn:awt:chooseL} the probability that a symmetric witness tree of order $L$ is activated is bounded above by $p_{awt }\leq n^{-\alpha}$.
\end{proof}

Since the probabilty for the activation of an asymmetric witness tree of order $L$ upper-bounds the probability of the bad event that some bin contains more than $L+3$ balls, Equation \ref{eqn:awt:result} also is an upper bound for the latter. Note that Equation \ref{eqn:awt:chooseL} can be written as
\begin{align*}
L &\geq \frac{\ln\log_2 n}{d\cdot \ln \Phi_d} + \frac{\ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d} + \frac{1}{d}\\
  &\geq \frac{\ln\ln n}{d\cdot \ln \Phi_d} + O\left(1\right)
\end{align*}
This proves Theorem \ref{theorem:agln}, under the assumption that all events are stochastically independent (Assumption \ref{assumption:independence}). \qed

\subsection{Non-distinct balls}
\label{sec:analysis:nondistinctBalls}
So far, we have assumed that all balls represented by nodes in a witness tree are stochastically independent (Assumption \ref{assumption:independence}). Obviously, to get a valid upper bound on the probability for the activation of a witness tree, the possibility of non-distinct balls has to be accounted for. In other words, the edge and leaf events of the witness tree are not independent anymore. Will start with a witness tree referencing non-distinct balls and prune it such that dependencies between edge and leaf events are removed. Note that pruning will reduce the size of the witness tree and thus increase the probability of its activation. To compensate for this loss in size, we start with a larger witness tree, which we call a \emph{full witness tree} and will show that, speaking informally, the pruning of many events is unlikely. Note that the technique presented in this section is applicable to both symmetric and asymmetric witness trees. 

\subsubsection{Full witness trees}
\label{sec:analysis:FullWT}
Let $\gamma \geq 2$ be a suitable constant. The root node of a \emph{full witness tree} has $\gamma$ children, denoted by $v_{c_i}$ with $1\leq i \leq \gamma$. The nodes $v_{c_i}$ have exactly one child each and the balls assigned to them are pairwise distinct. The root thus has $\gamma$ grandchildren, each of which is the root of a witness tree of order $L$. The time constraints and activation of a full witness tree is defined analogously to the previous variants.

\subsubsection{Construction}
\label{sec:analysis:constructionFullWT}
Assume a bin $x$ contains at least $L+3+\gamma$ balls at time $t$. We construct a witness tree as follows.
\begin{compactitem}
\item The root is not assigned any ball.
\item Each child of the root is assigned one of the topmost $\gamma$ balls in bin $x$, which are necessarily distinct.
\item For the assignment of nodes to the grandchildren of the root, consider a ball $b_c$ that is assigned to a child node $v_c$. Let the child node of $v_c$ be denoted by $v_g$. Since the ball $b$ was placed into bin $x$, at least one of its locations points to this bin $x$, which we call location $i$. Let $j = i+1 mod d$ be the ball's j-th location. Then, at the time of insertion of ball $b_c$, its j-th location points to a bin $y$ with at least $L+3$ other balls in it. We assign the topmost ball in bin $y$ to node $v_g$.
\end{compactitem}
Notice that the $\gamma$ additional balls in bin $x$ are assigned to the child nodes of the root only. 

\subsubsection{Pruning}
\label{sec:analysis:pruningFullWT}
To remove all dependencies in the witness tree, we have to find nodes that represent the same ball and prune the tree such that each ball is referenced only once. Thus, traverse the witness tree in breadth-first order. When we reach a node $v$ that represents a ball $b$ already encountered before, let $e = (v,u)$ be the edge connecting $v$ and its parent node $u$. We call $e$ a \emph{cutoff edge} and use it as a witness that one of the $d$ locations of the ball $b_u$ is the same as one of the locations of some other ball $b_{u'}$, represented by a node $u'$, which was already traversed. Furthermore, we remove the whole tree rooted at node $v$ from our witness tree. We continue this pruning until either the breath-first traversal terminates or until we have obtained $\gamma$ cut-off edges. Note that more than $\gamma$ cut-off edges cannot exist. When finished, the \emph{pruned witness tree} does not contain any ball twice and thus all its events are independent. 

\subsubsection{Probability of activation}
\label{sec:analysis:probabilityFullWT}
\begin{lemma}\label{lemma:fwt:activation}
For some suitable $\gamma \geq 2$ and large enough $L$, the activation of a prunded witness tree occurs with probability at most $n^{-\gamma +1 +o(1)}$.
\end{lemma}
If there are less than $\gamma$ cut-off edges, in at least one of the $\gamma$ symmetric or asymmetric witness trees contained in the full witness tree the referenced balls are distinct. Let this symmetric or asymmetric witness tree be denoted by $T$. Then, the probability for the activation of the full witness tree is upper-bounded by the probability for the activation of $T$, implying a probability for activation of this witness tree of $p = n^{- \alpha}$, as shown in Sections \ref{sec:analysis:probabilitySymWT} and \ref{sec:analysis:probabilityAsymWT}, respectively.

Thus, it remains to find an upper-bound for the activation of a full witness tree with $\gamma$ cut-off edges. Let $K$ be the number of references to balls by the full witness tree\footnote{Since no ball was assigned to the root node, the full witness tree consists of $K+1$ nodes} and assume that $K  \leq 2\cdot \gamma\left(\alpha+1\right)\cdot \log_2 n$, for some arbitrary $\alpha > 0$. Then, there are at most $w_p=K^\gamma$ possibilities to transform a full witness tree with $K+1$ nodes to a pruned witness tree with $\gamma$ cut-off edges. Fix the shape of the tree to be one of those. 

To bound the probability for the activation of all edge events, we need to specify which event is represented by an edge $e_i = (v_r, v_i)$, where $v_i$ with $1 \leq i \leq \gamma$ is a child of the root node $v_r$. Assume that the root node $v_r$ represents an arbitrary bin. Then the edge $e_i$ represents the event that one of the locations of the ball represented by node $v_i$ points to this bin. This implies that the probability of activation for an each $e_i$ is $p_{e_i} = 1/n \leq d/n$. 

Let $k$ be the number of balls referenced by the pruned witness tree and $l$ be the number of its leaf nodes. Then, by the same arguments as in Sections \ref{sec:analysis:probabilitySymWT} and \ref{sec:analysis:probabilityAsymWT}, we can conclude that there are at most $w=n^k$ assignments of balls to the nodes of the witness tree. Furthermore, the probability for the activation of all edge events is bounded from above by 
\begin{align*}
p_e = \left(\frac{d}{n}\right)^{k}
\end{align*}
while the probability for the activation of all leaf events is bounded by 
\begin{align*}
p_l \leq 3^{-d\cdot l}
\end{align*} 

The number of edges $k$ and the number of leaves $l$ in a pruned witness tree may be small compared to a symmetric or asymmetric witness tree, but we will use the cut-off edges to compensate. By definition (Section \ref{sec:analysis:pruningFullWT}), a cut-off edge $e=(v, u)$ represents the event that the ball $b_u$ represented by node $u$ shares a common location $i$ with another ball $b_{u'}$, represented by node $u'$. Since $u'$ has already been visited by the time the breadth-first traversal reaches node $v$, it is part of the pruned witness tree and was assigned one of $k\leq K$ balls. The probability that location $i$ of ball $b_u$ points to one of the $d$ locations of ball $b_{u'}$ is at most $d/n$. The probability that there are $\gamma$ cut-off edges is thus at most 
\begin{align*}
p_c &= \left(k\cdot \frac{d}{n}\right)^\gamma \\
    & \leq \left(K\cdot \frac{d}{n}\right)^\gamma
\end{align*}

Putting it all together, we get 
\begin{align*}
p_{pwt} &= w_p\cdot w \cdot p_e \cdot p_l \cdot p_c \\
		&= K\gamma\cdot n^{k+1}\cdot \left(\frac{d}{n}\right)^k\cdot3^{-d\cdot l} \cdot \left(K \frac{d}{n}\right)^\gamma\\
		&= n \cdot d^k\cdot 3^{-d\cdot l} \cdot \left(K^2 \frac{d}{n} \right)^\gamma 
\end{align*}

Using $K \leq 2l$, $d^2 \leq 3^d$ and $K \leq 2\gamma \cdot \left( \alpha +1 \right) \cdot \log_2 n$ this becomes
\begin{align*}
p_{pwt} &\leq n\cdot \left(\frac{\left(2\gamma \cdot \left(\alpha +1 \right) \cdot \log_2 n\right)^2\cdot d}{n}\right)^\gamma \\
        &= n^{-\gamma +1} \cdot n ^{\log_n\left( \left(2\gamma\cdot \left(\alpha+1\right)\cdot \log_2n \right)^{2\gamma} \cdot d^\gamma \right)}\\
        &= n^{-\gamma+1+o\left(1\right)}
\end{align*}
\todo{Where are our $L$ and $\alpha$ here?}
\qed
 
\section{Conclusion}
\label{sec:conclusion}
Balls-into-bins games have many applications in Computer Science and have been studied extensively. The single-choice balls-into-bins strategy chooses a bin uniformly and independently at random for each ball. The classical result for this scheme upper-bounds the maximum load with 
\begin{align*}
\load = \frac{m}{n} + \Theta\left(\sqrt{\frac{m \cdot \log n}{n}}\right).
\end{align*}
w.h.p.
In this paper we have presented two different miltiple-choice balls-into-bins placement algorithms: The Greedy-scheme due to Azar et al.\cite{ABKU99}, which achieves a maximum load of 
\begin{align*}
\load = \frac{m}{n} + \frac{\ln\ln n}{\ln d} + O(1)
\end{align*}
w.h.p.~and the Always-go-left scheme due to V\"ocking \cite{VOC03}, which achieves a slightly better maximum load of
\begin{align*}
\load = \frac{m}{n} + \frac{\ln \ln n}{d\cdot \ln \Phi_d} + O(1)
\end{align*}
w.h.p.~for all $m$. We used \emph{witness trees}, a very useful technique in the balls-into-bins context, to prove these upper-bounds in detail.


\appendix
\section{Generalized Fibonacci numbers}
\label{sec:fibonacci}
We define $d$-ary Fibonacci numbers as follows. For $k \leq0$, $F_d(k) = 0$, $F_d(1) = 1$ and for $k \geq 1$,

\begin{align*}
F_d(k) = \sum_{i=1}^{d}F_d(k-i)
\end{align*}
Furthermore, define 
\begin{align*}
\Phi_d = \lim_{k \rightarrow \infty} \sqrt[k]{F_d(k)} \neqn{goldenFib}
\end{align*}
Notice that $F_2$ corresponds to the usual Fibonacci numbers, while $\Phi_2 $ corresponds to the golden ratio \cite{Knuth73}, which is generalized to $\Phi_d$, for which 
\begin{align*}
i, j \in \mathbb{N}, i < j \Rightarrow \Phi_i < \Phi_j
\end{align*}
and 
\begin{align*}
\lim_{d\rightarrow \infty} \Phi_d = 2
\end{align*}
hold.

\section{Asymptotic notations}
\label{sec:asymptotic}
We use the following definitions for the asymptotic notations $O(\cdot)$, $o(\cdot)$ and $\Theta(\cdot)$.

\begin{align*}
f(n) \in o(g(n)) &\Leftrightarrow \forall \epsilon > 0 \exists n_o\forall n> n_0: |f(n)|\leq \epsilon \cdot |g(n)|\\
f(n) \in O(g(n)) &\Leftrightarrow \exists c > 0 \exists n_o\forall n> n_0: |f(n)|\leq c \cdot |g(n)|\\
f(n) \in \Omega(g(n)) &\Leftrightarrow \exists c > 0 \exists n_o\forall n> n_0: c \cdot |g(n)| \leq |f(n)| \\
f(n) \in \Theta(g(n)) &\Leftrightarrow f(n) \in \Omega(g(n)) \land f(n)\in O(g(n))
\end{align*}

The following theorem is useful for understanding the asymptotic bounds presented throughout the paper.
\begin{theorem}\label{theorem:asymptotic}
\begin{align*}
f(n) = g(n) \cdot (1+o(1)) \Rightarrow f(n) \in \Theta(g(n)) 
\end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
f(n) &= g(n)+o(1)\cdot g(n) \\
&\Rightarrow f(n) \in \Omega(g(n)) \land f(n) \in O(g(n)) \\
&\Rightarrow f(n) \in \Theta(g(n))
\end{align*}
However, the reverse
\begin{align*}
f(n) \in \Theta(g(n)) \Rightarrow f(n) = g(n) \cdot (1+o(1)) 
\end{align*}
is not true. As a counter example, consider $f(n) = n$ and $g(n) = 2n$.
\end{proof}

\section{A bit of history}
\label{sec:historyOfGreedy}
The history of proving the remarkable property of the two-choice paradigm (see Theorem \ref{theorem:greedy}) is in itself quite interesting. It was first proven by Azar et al. \cite{ABKU99} for the case $m = n$. In the same paper, they claimed that the bound holds for arbitrary $m$. Unfortunately, their analysis breaks down for the heavily loaded case, where $m \in \Omega\left(n  \log n\right)$. Berenbrink at. all \cite{BFZR08} were the first to proof that the same bound indeed holds for the heavily loaded case. Their proof is fairly complex and a simpler analysis may be found in \cite{TW13}, which also provides a tight lower bound for the heavily loaded case. Curiously, the case $d = 2$ was implicitly proven by Karp et al. in the context of simulating PRAMs on distributed memory machines\cite{KLM92}.

As discussed in Section \ref{sec:AlwaysGoLeft}, V\"ocking \cite{VOC03} was able to slightly improve the bound as well as to show that the \emph{Always-go-left} strategy that he introduces is optimal up to additive constants.


%bibliography
\bibliographystyle{acm}
\bibliography{sources} 
\end{document}
