\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{comment}
\usepackage{hyperref}
%page boarders
\usepackage[vmargin=3cm, hmargin=3cm]{geometry}
\usepackage{float}
\floatstyle{plaintop}
\usepackage{paralist} %compactitem
\usepackage{fixltx2e} %some latex fixes
\usepackage{microtype} %Subliminal refinements towards typographical perfection
\setlength{\emergencystretch}{2em}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs} %Support use of the Raph Smith’s Formal Script font in mathematics
\usepackage[sc]{mathpazo} %mathematical fonts
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[margin=10pt, font=small, labelfont=bf]{caption}

\usepackage{color}
\newcommand\todo[1]{\textcolor{red}{(#1)}}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{invariant}{Invariant}
\begin{document}

\title{The Power of Two Random Choices\\ 
\large A follow up seminar to the Parallel Algorithms lecture\footnote{Jesper Larsson Träff, lecture ``Parallel Algorithms'', 2012 winter term at TU Wien.}}
\author{Martin Kalany, 0825673}

\maketitle


\begin{abstract}

\end{abstract}

%\tableofcontents

\section{Introduction}
\label{sec:intro}
Suppose that $m$ balls are thrown into $n$ bins, according to some strategy. We are interested in the \emph{number of balls in the fullest bin}, also called the \emph{maximum load}. Equivalently, one can look at the \emph{addative gap} of the allocation, that is the difference of the number of balls in the fullest bin and the average amount of balls in the bins. These \emph{balls-into-bins games} or \emph{allocation problems} have been studied extensively in the probability literature (see e.g., \cite{JK77}). In the context of Computer Science, the balls-into-bins model is useful to study e.g., strategies for distributing \emph{tasks} (balls) to \emph{processors} (bins) as evenly as possible without having to query all processors for the number of tasks currently assigned to them.

A simple strategy for the balls-into-bins problem is to choose one bin uniformly and independently at random for each ball, which we will call the \emph{single-choice balls-into-bins} strategy. Using this strategy, the maximum load of the fullest bin will be $\frac{m}{n} + \Theta\left(\sqrt{\frac{m \cdot \log n}{n}}\right)$ with high probability\footnote{We say that an event $\mathcal E$ occurs \emph{with high probability} (w.h.p.) if $\Pr\left[\mathcal E \right] = 1 - o(1)$, except when stated otherwise.}~\cite{RS98}.

Consider a variation of the above \emph{single-choice balls-into-bins} strategy: For each ball, chose $d \geq 2$ bins independently and uniformly at random and place the ball into the \emph{least full bin}\footnote{If there are multiple bins with the same minimal load, chose one among them arbitrarily.}. This simple \emph{multiple-choice balls-into-bins} scheme, which we will call the \emph{Greedy} algorithm, achieves a maximum load of $\frac{m}{n} + \Theta\left(\frac{\log \log n}{\log d}\right)$ w.h.p.~\cite{ABKU99}~\cite{BCSV06}. This apparently minor change introduces a small amount of choice which, compared to the single-choice scheme, leads to a significant reduction in the maximum load. 
\begin{comment}
Curiously, the exponential decrease in the gap is already achieved having just two choices (that is, $d=2$). Each additional choice decreases the bound for the maximum gap by just a constant factor~\cite{MRS01}. 
\end{comment}
Extensive research following the influental paper by Azar et. al \cite{ABKU99}, who were the first to explicitly prove this phenomenon, suggests that this \emph{two-choice paradigm} is not just an artificat of the simple balls-into-bins model, but a general principle applicable to a variety of problems. 

Condsider a slightly more involved strategy, which we will call the \emph{Always-go-left} scheme: Suppose that the $n$ bins are partitioned evenly into $d$ groups, each of size $\lfloor \frac{n}{d}\rfloor$ or $\left\lceil\frac{n}{d}\right\rceil$ \todo{simplify to n/d?}. We select one bin from each group uniformly and independently at random and put the ball into the least full of those $d$ bins. In case a tie occurs, put the ball into the \emph{left-most} bin. That is, assuming a fixed ordering of bins $b_i$ for $1 \leq i \leq n$, from among the $d' \leq d$ bins with same minimal load, we put the ball into the bin with smallest index $i$. Somewhat surprisingly, V\"ocking \cite{VOC03} showed that such a strategy employing \emph{asymetric tie-breaking} in combination with \emph{partitioning}\todo{check} reduces the bound for the gap to $\Theta \left( \frac{\log\log n}{d  \cdot \log \Phi_d}\right)$ \todo{$\Phi_d$ is undefined}.

The paper is organized as follows: We will first introduce different variations of the \emph{balls-into-bins problem} in Section \ref{sec:classification} to provide some overview of different directions in research as well as possibilities of abstraction from applications, some of which are listed in Section \ref{sec:applications}. Sections \ref{sec:single-choice} to \ref{sec:multiple-choice} will deal with the most basic variation of the balls-into-bins problem, namely the sequential, finite and static variant (see Section \ref{sec:classification} for details). For comparison and as a starting point, Section \ref{sec:single-choice} provides the results for the single-choice balls-into-bins problem. \todo{TODO: multiple-choice, analysis, mention that witness tree method very helpful}  See also \cite{MRS01} an overview of proof techniques for the given problem.  

\begin{comment}
In Section \ref{sec:strategies}, we will provide asymptotic bounds for the addative gap of this strategy, which we will then use to devise more complex strategies that significantly reduce these bounds. 
\end{comment}

\begin{comment}
These result may sometimes seem surprising, which is why we will denote a significant part of this article to demonstrate proofs in detail. This proof technique may be useful for devising asymptotic bounds for the additive gap of different balls-into-bins scenarios and strategies.
\end{comment}


\section{Classification}
\label{sec:classification}
Several different variations of this problem exist, which may be classified by the following properties:
\begin{compactitem}
\item In a \emph{sequential process}, the balls arrive one after another. A  ball is dealt with only after the previous ball has been placed. Contrary, in a \emph{parallel process}, balls arrive in sets and the balls in one set are dealt with simultaneously \cite{ABS98}. 
\item In a \emph{static problem}, the balls are placed into the bins and no balls are ever removed. In a \emph{dynamic problem}, balls can also be removed from the system. In a simple \emph{random deletions} model, for each newly inserted ball a ball choosen uniformly and independently at random from among the balls currently in the bins is removed from the system \cite{ABKU99} \cite{MRS01}. More elaborate shemes exists: An \emph{oblivious adversery} specifies the sequence of insertions and deletions of balls in advance, without any knowledge of the placement of the balls \cite{CFM+98}. Another approach was studied by Adler et. al \cite{ABS98} in the context of the parallel balls-into-bins problem: In each bin, the balls are stored in FIFO-order and at every time step the first ball in each of the bins is deleted. 
\item A \emph{finite process} is analyzed for a given time interval that is known beforehand, whereas for an \emph{infinite process}, a placement and a deletion process is defined over an infinite time span \cite{ABS98}.
\end{compactitem}


We will distinguish two cases with respect to the number of balls $m$. In the \emph{lightly loaded case}, $m$ is in the order of $n$, whereas in the \emph{heavily loaded case}, $m$ is aribtrarily large. For clarity, we will assume that $m = n$ in the slightly loaded case and that $m \ll n$\todo{TODO} \footnote{Let $f$, $g$ be arbitrary functions. Then, $f \ll g$ means $f = o(g)$ and $ f \gg g$ means $f = \omega(g)$} in the heavily loaded case. 

\section{Applications}
\label{sec:applications}
The two-choice paradigm applied to the balls-into-bins model has several useful applications, some of which we will outline here. The applications in \emph{Hashing} (Section \ref{sec:hashing}), \emph{Online Load Balancing} (Section \ref{sec:loadbalancing}) and \emph{Emulation of shared memory machines on distributed memory machines} \ref{sec:DMM}) share a close relation to load balancing. Section \ref{sec:circuitrouting} applies the balls-into-bins model and the two-choice paradigm in a different context, namely that of routing circuits in multi-stage interconnection networks with minimal congestion.

\subsection{Hashing}
\label{sec:hashing}
A simple hash table implementation typically uses a single hash function to map elements to entries in a table. If more than one element is mapped to the same entry, all elements mapped to this table entry are stored in a \emph{chain}, usually implemented as a linked list. The maximum time to search for an element is then proportional to the length of the longest chain in the hash table. 

It is easy to see how the balls-into-bins idea can be applied: Assuming a perfect hash function\footnote{i.e., each of the $n$ elements is mapped to one of the $n$ table entries uniformly and independently at random}, the elements are balls to be mapped to table entries or bins, implying a worst case look-up time of $\Theta\left(\frac{\log n}{\log \log n}\right)$ w.h.p. Applying the two-choice paradigm by using two hash functions and mapping elements to the least full of two table entries implies a bound of $\Theta\left(\log \log n\right)$ w.h.p. This approach is easy to parallelize, it does not require re-hashing of data and requires only two hash functions \cite{ABKU99} \cite{MRS01}.

\subsection{Online load balancing}
\label{sec:loadbalancing}
The following simple scenario is a typical application of online load balancing: Consider $n$ servers\footnote{Such as file-, database- or network servers} and $m$ requests issued by clients that need to be handled by these servers. For simplicity, assume that each task can be handled by any server and that all tasks are of the same size, i.e., require the same amount of execution time.

To maximize the throughput of the servers, a uniform distribution of tasks to servers is desireable. This can easily be achieved by a central dispatcher. However, such a central dispatcher poses a bottleneck within a distributed system and querying $m$ servers for their current load involves significant overhead, e.g., sending a message to each server and waiting for the reply and more efficient solutions are neccessary. As with hashing, the balls-into-bins model can be applied easily: we need to assign tasks (the balls) to servers (the bins). Assuming tasks are assigned to servers uniformly and independently at random, the two-choice paradigm is directly applicable: If each client samples two random servers for their load and sends issues its request to the least loaded server, the overhead is comperatively small and the difference in load on the $n$ servers is bounded by $\Theta\left(\log \log n\right)$ w.h.p. \cite{KLM92} \cite{RS98} \cite{MRS01}.  

\subsection{Emulation of shared memory machines on distributed memory machines}
\label{sec:DMM}
The \emph{Parallel Random Access Machine} model\footnote{as defined in the lecture or in~\cite{P03}}, or \emph{PRAM} for short, is a high level abstraction of a shared memory machine, in which memory access by any processor to any memory location is assumed to require constant time. It is especially useful for the study of parallel algorithms. However, a PRAM is much harder to realize in hardware than a \emph{Distributed Memory Machine} (DMM). Thus, ways to emulate a PRAM on a DMM have been studied extensivly. Such an emulation distributes the processors and memory cells of the PRAM to the processors and memory modules (typically one per processor) of the DMM with the goal to reduce the slowdown of the emulation, which is the time the DMM requires to simulate one step of the PRAM. Each memory modul can handle only one memory access at a time step. More accesses to the same module will result in memory contention. The balls-into-bins idea is useful for managing communication between the processors and the memory modules such that memory contention is minimal. For details, see Karp et. al \cite{KLM92} and the papers referenced there. 
 
\subsection{Low congestion circuit routing}
\label{sec:circuitrouting}
In multi-stage interconnection networks, virtual circuit-switching is used to route communication. For simplicity, we consider the \emph{permutation routing problem}, where one request originates at each of the $n$ input nodes and one request is destined for each of the $n$ output nodes and assume a \emph{butterfly network} as the underlying interconnection network (see \cite{CLR09} for a detailed definition). Then, congestions is the maximum number of paths that are routed through a network link. The goal of a \emph{circuit routing algorithm} is to to allocate paths for all $n$ communication requests such that congestion is minimal.

In this context, \emph{Valiant's paradigm} \cite{V82} states that any such permutation routing problem can be solved by transforming it into two problems: First, find a path for each request to an intermediate destination chosen uniformly and independently at random. Next, find a path from the intermediate to the actual destination. This routing technique is analogous to the single-choice balls-into-bins problem where each communication request (or ball) chooses an intermediate destination (a bin) at random. In this analogy, the congestion achieved by Valiant's paradigm corresponds to the maximum load in the balls-into-bins model, thus implying a bound of $\Theta\left(\log \log n\right)$ on the congestion w.h.p. \cite{CMM+98} \cite{MRS01}. 

Applying the two-choice paradigm in this context is somewhat more involved and beyond the scope of this paper. The interested reader may find more details in \cite{CMM+98}.

\section{Single-choice balls-into-bins}
\label{sec:single-choice}
We will now consider the most basic variant of the balls-into-bins problem: Namely the static, finite and sequential variant. Thus, the balls are assumed to arrive sequentially, no strategy for deleting balls in the bins has to be considered and we are interested in a bound for the maximum load or gap only after all $m$ balls have arrived. 

In the \emph{single-choice balls-into-bins} strategy, each ball is placed into a bin chosen uniformly and independently at random. In other words, a ball gets placed into a certain bin with probability $\frac{1}{n}$.

The fullest bin contains
\begin{align}
\Theta \left( \frac{\log n}{\log \log n} \right)
\end{align} 
balls in the lightly loaded case w.h.p. Since the average load of the bins is 1, this is also a bound for the gap. 
%More accurately, the number of balls in the bin with the most balls is $\Gamma^{-1}\left(n\right)\left(1+O\left(\frac{1}{\log \Gamma^{-1}\left(n\right)}\right)\right)$ \cite{G91}. %\todo{really include Gonnet?}
In the \emph{heavily loaded case} the maximum load is 
\begin{align}
\frac{m}{n} + \Theta\left(\sqrt{\frac{m \cdot \log n}{n}}\right)
\end{align} 
w.h.p.~\cite{RS98}.

\section{Multiple-choice balls-into-bins}
\label{sec:multiple-choice}
The \emph{single-choice balls-into-bins strategy} (Section \ref{sec:single-choice}) chooses one bin uniformly and independently at random for each ball. A \emph{multiple-choice balls-into-bins strategy} generalizes this idea by  choosing $d \geq 2$ bins and placing the ball into the least full bin. Depending on how the $d$ locations are sampled, we distinguish three classes of placement algorithms \cite{VOC03}. 

Thus, let $B$ be the set of bins $\{1,2...n\}$, $\Omega = B^{d}$ be the sample space and $\mathcal{F} = 2^{\Omega}$ be the set of measurable events. We are only interested in events that represent a set of bins of size $d$. Thus, let $\mathcal{F}_d = \{\mathcal{E} \in \mathcal{F}: \left\vert \mathcal{E} \right\vert = d\}$ and set $\Pr\left(\mathcal{E}  \right) = 0$  $\forall \mathcal{E} \in \mathcal{F} \setminus \mathcal{F}_d $. The three classes of algorithms are distinguished by the probability function $\Pr\left(\mathcal{E}\right)$ for events $\mathcal{E} \in \mathcal{F}_d$:
\begin{compactitem}
\item  \emph{Uniform and independent.} Choose each of the $d$ bins for a ball uniformly and independently at random from the set of bins $B$. This implies 
\begin{align}
\forall \mathcal{E} \in \mathcal{F}_d: \Pr\left(\mathcal{E}\right) = \frac{1}{n ^{d}}
\end{align}
\item Class 2: \emph{Non-uniform and independent.} Choose the $i$th bin from $B$ independently at random according to some probability function
\begin{align}
\Pr: B \rightarrow \left[0,1\right]
\end{align}

\item Class 3: \emph{Non-uniform and dependent.} Choose the $d$ bins for a ball from $\Omega$ at random:
\begin{align}
\Pr: B^{d} \rightarrow \left[0,1\right]
\end{align}
\end{compactitem} 
 
We will discuss two different multiple-choice placement strategies: The \emph{Greedy} algorithm (Section \ref{sec:greedy}) belongs to class 1, whereas the \emph{Always-go-left} scheme (Section \ref{sec:AlwaysGoLeft}) uses a non-uniform and independent selection of $d$ bins and thus belongs to class 2.

\subsection{The Greedy scheme}
\label{sec:greedy}

Consider a variation of the above \emph{one-choice balls-into-bins} strategy, which we will call the \emph{Greedy} scheme: For each ball, choose $d \geq 2$ bins independently and uniformly at random and place the ball into the \emph{least full bin}. If there are multiple bins with the same minimal load, chose one among them arbitrarily. Note that the $d$ selected bins are not necessarily distinct. 

\todo{specify deletions; fix intro text since we now do use deletions}

\todo{use theorem from V\"ocking?}

\begin{theorem}
\label{theorem:greedy}
Consider any sequence of insertions and deletions s.t.~at most $hn$ balls are in the bins at any time. If the balls are placed into bins by the Greedy algorithm, at any time $t$ the maximum load is at most 
\begin{align}
\frac{m}{n} + \Theta\left(\frac{\log \log n}{\log d} \right)
\end{align}
w.h.p.~at any time $t$ \cite{ABKU99} \cite{BCSV06}.

\end{theorem}

This implies a gap of $\Theta\left(\log \log n \right)$ for arbitrarily large $m$, which is remarkable in several ways:
\begin{compactitem}
\item The apparently small change made for the \emph{Greedy} algorithm compared to the \emph{single-choice balls-into-bins} scheme results in an exponential decrease of the gap, even for $d=2$. 
\item Each additional choice ($d > 2$) decreases the gap by only a constant factor \cite{MRS01}. 
\item For the \emph{heavily loaded case}, the resulting gap of the \emph{multiple-choice balls-into-bins} strategy does not depend on the number of balls $m$. In contrast, the bound for the \emph{one-choice balls-into-bins} scheme diverges with $m$.
\begin{comment}
\todo{the following is only true for class 1 algorithms}
\item The given bounds are \emph{tight}, meaning that no other strategy that places each ball into one of $d$ randomly selected bins achieves a gap that is asymptotically lower.
\end{comment}
\end{compactitem}

We will provide a detailed proof of this property in Section \ref{sec:analysis}.

\subsubsection{History}
\label{sec:historyOfGreedy}
The history of proving this remarkable property of the two-choice paradigm is in itself quite interesting. It was first proven by Azar et al. \cite{ABKU99} for the case $m = n$. In the same paper, they claimed that the bound holds for arbitrary $m$. Unfortunately, their analysis breaks down for the heavily loaded case, where $m = \omega\left(n\right)$. Berenbrink at. all \cite{BFZR08} were the first to proof that indeed the same bound holds for the heavily loaded case\footnote{However, they use a slightly different definition for w.h.p.: An event $\mathcal E$ occurs with high probability if $\Pr\left[\mathcal E \right]  \geq 1- n^{-\alpha}$, for an arbitrarily chosen constant $\alpha \geq 1$.}. A simplified proof may be found in \cite{TW13}. Curiously, the case $d = 2$ was implicitly proven by Karp et al. in the context of simulating PRAMs on distributed memory machines\cite{KLM92}. As we will see in the next section, V\"ocking was able to slightly improve the bound as well as to show that the \emph{Always-go-left} strategy that he introduces is optimal up to additive constants \cite{VOC03}.


\subsection{The Always-go-left scheme}
\label{sec:AlwaysGoLeft}
The \emph{Greedy} strategy presented in Section \ref{sec:multiple-choice} assumes that $d$ bins are chosen uniformly and independently at random. V\"ocking \cite{VOC03} was the first to study the impact of non-uniform and dependent selections of the $d$ bins. Surprisingly, a multiple-choice balls-into-bins strategy with a \emph{non-uniform} and independent selection of the $d$ bins, such as the \emph{Always-go-left} algorithm introduced in \cite{VOC03}, achieves better\cite{BCSV06} load balancing than the \emph{Greedy} scheme (Section \ref{sec:greedy}). Furthermore, V\"ocking proved that the \emph{Always-go-left} algorithm is optimal up to additive constants.

The \emph{Always-go-left} algorithm works as follows. Partition the $n$ bins into $d$ groups, each of size $\left\lfloor \frac{n}{d}\right\rfloor$ or $\left\lceil\frac{n}{d}\right\rceil$ . We select one bin from each group uniformly and independently at random and put the ball into the least full of those $d$ bins. In case a tie occurs, put the ball into the \emph{left-most} bin. That is, assuming a fixed ordering of bins $b_i$ for $1 \leq i \leq n$, from among the $d' \leq d$ bins with the same minimal load, put the ball into the bin with smallest index $i$.

\begin{theorem}
\label{theorem:agln}
If $n$ balls are placed into $n$ bins using the Always-go-left scheme, the number of balls in the fullest bin will be 
\begin{align}
\frac{\ln\ln n}{d \cdot \ln \Phi_d} \pm \Theta(1)
\end{align}
w.h.p.
\end{theorem}

\begin{theorem}
\label{theorem:algm}
Consider any sequence of insertions and deletions s.t.~at most $hn$ balls are in the bins at any time. If the balls are placed into bins by the Greedy algorithm, at any time $t$ the maximum load is at most 
\begin{align}
\frac{\ln\ln n}{d \cdot \ln \Phi_d} + O(h)
\end{align}
w.h.p.
\end{theorem}

\section{Analysis}
\label{sec:analysis}
In this section, we will prove Theorems \ref{theorem:greedy}, \ref{theorem:agln} and \ref{theorem:algm} in detail, using \emph{witness trees} to provide an upper bound for the event that a bin contains too many balls. Our analysis closely follows the proofs in \cite{VOC03}. \todo{is it enough to cite this once for the whole section?}

When the maximum load exceeds some given threshold value we say that a \emph{bad event} occurs and construct an \emph{activated witness tree}. In other words, the activation of a witness tree is implied by a bad event. The probability for the existance of such an activated witness tree is an upper-bound for the probability that a bad event occurs.

Throughout this section, we donate the $d$ bins that are selected for a ball as the \emph{locations} of a ball. That is, before inserting a ball $d$ locations are generated and the ball is then inserted into one of those.

We will prove the bounds for $m=n$ for both the Greedy and the Always-go-left scheme in Sections \ref{sec:analysis:greedy} and \ref{sec:analysis:alg}, respectively. However, we use a simplifying assumption \ref{assumption:independence} in those two sections to provide a more clear presentation of the proof technique. Section \ref{sec:analysis:nondistinctBalls} will extend these proofs to remove the simplifying assumption we make initially. Section \ref{sec:analysis:moreBalls} will extend the proof to cover the heavily loaded case where $m > n$ as well.


\subsection{Preliminaries}
\label{sec:preliminaries}
\todo{where to put this? required for  bound of always-go-left in intro}
We define $d$-ary Fibonacci numbers as follows. For $k \leq0$, $F_d(k) = 0$, $F_d(1) = 1$ and for $k \geq 1$,

\begin{align}
F_d(k) = \sum_{i=1}^{d}F_d(k-i)
\end{align}
Furthermore, define 
\begin{align}\label{eqn:goldenFib}
\Phi_d = \lim_{k \rightarrow \infty} \sqrt[k]{F_d(k)}
\end{align}
Notice that $F_2$ corresponds to the usual Fibonacci numbers, while $\Phi_2 $ corresponds to the golden ratio \cite{Knuth73}, which is generalized to $\Phi_d$, for which 
\begin{align}
i, j \in \mathbb{N}, i < j \Rightarrow \Phi_i < \Phi_j
\end{align}
and 
\begin{align}
\lim_{d\rightarrow \infty} \Phi_d = 2
\end{align}
hold.

\subsection{Greedy}
\label{sec:analysis:greedy}
\subsubsection{Symmetric witness trees}
\label{sec:analysis:definitionSymWT}
A \emph{symmetric witness tree} of order $L$ is a complete $d$-ary tree with $d^{L}$ leaves. Each node $v$ represents a ball $b_v = \mathrm{ball}(v)$ . The same ball may be represented by several nodes. The nodes and edges of the witness tree represent events that may occur or not. For simplicity, we assume that all events represented by a witness tree are stochastically independent.  

We define edge and leaf events:
\begin{compactitem}
\item \emph{Edge event:} Consider some edge $e = (u,v)$ of the witness tree, s.t.~$v$ is the $i$th child of $u$. Then $e$ represents the event that the $i$'th location of the ball $b_u = \mathrm{ball}(u)$ points to the same bin as one of the locations of the ball $b_v = \mathrm{ball}(v)$. 
\todo{illustrate}
\item \emph{Leaf event:} A leaf node $v$ of the witness tree represents the event that each of the $d$ locations of the ball $b_v = \mathrm{ball}(v)$ points to a bin that contains at least three additional balls (i.e., balls that are not represented by a node of the tree) at the time of the insertion of the ball $b_v$.
\todo{illustrate}
\end{compactitem}

The following constraints have to be fullfilled by an assignment. The ball $b_r$ that is represented by the root node $r$ has to exist at timestep\footnote{As defined in Theorem \ref{theorem:greedy}} $t$ and each ball $b_v$ represented by a node $v \neq r$ and parent node $u$ has to exist when the ball $b_u = \mathrm{ball}(u)$ is inserted.

The definition for leaf events refers to bins containing some number of balls at some specified time. To avoid dependencies, the crucial trick is that edge events are defined in terms of the alternative locations of balls, not by their finally assigned bin. 

Assume the bad event that some bin contains more than $L+3$ balls at time $t$ occurs. We will use an activated symmetric witness tree of order $L$ to witness this event and provide an upper-bound for the occurrence of the event. Thus, we have to show that the existence of a bin $x$ containing at least $L+4$ balls implies the existence of an activated witness tree of order $L$. 

\subsubsection{Construction}
\label{sec:analysis:constructionSymWT}
We construct a witness tree for the event that some bin contains $L+4$ balls as follows:
\begin{compactenum}
\item The topmost ball $b_1$ in bin $x$ is assigned to the root node $v_r$. Note that $b_1$ is the last ball that was inserted into the bin $x$.
\item Since the Greedy-scheme puts ball $b_1$ into bin $x$ only if each of the $d$ sampled locations contain at least the same number of balls at bin $x$, we can conclude that each of the $d$ locations contain at least $L+3$ balls at the time ball $b_1$ was inserted. Thus, the $d$ children of the root node are assigned the topmost balls of these $d$ bins.
\item Consider these child nodes and continue the assignment of the previous step recursively until the leaf nodes are reached.  
\end{compactenum}

By construction, all edge events of the witness tree are activated. Note that the ball assigned to the root node is on top of at least $L+3$ balls. The balls assigned to the root's child node are each on top of $L+2$ or more balls and so on. It follows that each ball assigned to a leaf node is on top of at least 3 other balls, implying that the $d$ locations of each leaf node point to bins with at least 3 balls in each. This implies the activation of all leaf nodes of the witness tree.

\subsubsection{Probability of activation}
\label{sec:analysis:probabilitySymWT}
In this section, we derive the probability that some witness tree of order $L$ is activated, which gives an upper bound for the probability that the maximum load is $L+3$ at some time $t$. The probability $p_a$ that some witness tree of order $L$ is activated is bounded by the number of witness trees $w$ times the upper bound on the probability $p_1$ that a specific witness tree is activated.

We will make use of Assumption \ref{assumption:independence} in the analysis. 
\begin{assumption}
\label{assumption:independence}
All events represented by the nodes and edges of a witness tree are stochastically independent. 
\end{assumption}
In other words, we assume that no ball is referenced by more than one node of a witness tree. Obviously, a correct analysis will have to consider balls assigned to more than one node, but this simplifying assumption is useful to highlight the differences between the symmetric and asymmetric allocation schemes. Section \ref{sec:analysis:nondistinctBalls} will extend the analysis to witness trees with reduntant balls.

The number of different witness trees $w$ is given by the number of ways to assign balls to its nodes. The ball assigned to the root node can be chosen from at most $n$ balls, since by definition (Theorem \ref{theorem:greedy}), at most $n$ balls are in the bins at any time $t$ and a ball assigned to a node has to be in some bin at time $t$. Each child node of the root is assigned one out of at most $n$ balls too by the same argument. Applying this argument for each level of the witness tree, there are at most $w = n^k$ possibilities to assign balls to a witness tree, with $k = d^{L+1}-1$ being the number of nodes in a $d$-ary witness tree of order $L$.
 
An edge\footnote{where $v$ is the $i$'th child of $u$} $(u, v)$ is activated with probability at most $d/n$, which is the probability that the $i$'th location of the ball $b_u = \mathrm{ball}(u)$ points to a particular location of the ball $b_v = \mathrm{ball}(v)$ times the number of locations $d$. This implies a probability of at most
\begin{align}
p_e = \left(\frac{d}{n}\right)^{k-1}
\end{align} 
or the event that all $k-1$ edges of a witness tree are activated. Note that  the assumption that all balls are distinct (Assumption \ref{assumption:independence}) implies that all edge events are independent.

The probability that a leaf event is activated is bounded above by $3^{-d}$. Recall that by the definition of leaf events, each of the $d$ locations of the ball $b_{vl} = \mathrm{ball}(v_l)$ has to point to a bin with at least 3 additional balls in it. The number of those bins is bounded by $\lfloor n/3 \rfloor$ at all time steps $t$, implying a probability of at most 
\begin{align}
\frac{n/3}{n} = \frac{1}{3}
\end{align}
that one such bin is chosen. This implies a probability of at most $3^{-d}$ that all $d$ bins are chosen among those with at least 3 additional bins in them. Thus, the probability that all leaf events are activated is at most $p_l = 3^{-d\cdot l}$, where $l = d^{L}$ is the number of leaves in a witness tree.

Thus, the probability that an activated symmetric witness tree exists at time $t$ is at most 
\begin{align}
p_{swt} &= w \cdot p_e \cdot p_l \\
		&= n^{k} \cdot \left(\frac{d}{n}\right)^{k-1} \cdot 3^{-d \cdot l} \\
		&= n \cdot d^{k-1} \cdot 3^{-d \cdot l}
\end{align}

Using $k \leq 2l$ we get
\begin{align}
p_{swt} &\leq n \cdot d^{2\cdot l} \cdot 3^{-d \cdot l} \\
		&= n \cdot \left(d^2\right)^l \cdot \left( 3^{-d} \right)^l
\end{align}
which we simplify by using $2d^2 \leq 3^d$
\begin{align}
p_{swt} &\leq n \cdot \left(\frac{3^d}{2}\right)^l \cdot \left( 3^{-d}\right)^l \\
		&= n \cdot 2^{-l} \cdot 3^{d\cdot l} \cdot 3^{-d \cdot l} \\
		&= n\cdot 2^{-l}		\\
		&= n\cdot 2^{-d^L}		
\end{align}

For any constant $\alpha > 0 $, let
\begin{align}
L &\geq \log_d\log_2 n + \log_d\left(1+\alpha\right)
\end{align}
and note that
\begin{align}
L &\geq  \frac{\ln\log_2 n}{\ln d} + \frac{\ln(1+\alpha)}{\ln d} \\
   &= \frac{\ln \log_2 n}{\ln d} \cdot \left(1 + \frac{\ln\left(1+\alpha\right)}{\ln \log_2 n}\right) \\
  &\geq  \frac{\ln \ln n}{\ln d} \cdot \left( 1+ o\left( 1\right)\right)
\end{align}

Then, 
\begin{align}
p_{swt} &\leq n \cdot 2^{{-d}^{\log_d\log_2 n + \log_d\left(1+\alpha\right)}} \\
		&= n \cdot 2^{-{d}^{\log_d\log_2 n}\cdot d^{\log_d \cdot\left(1+\alpha\right)}} \\
		&= n \cdot \left( 2^{-\log_2 n \cdot \left(1+ \alpha \right)} \right) \\
		&= n \cdot n^{-\left(1+\alpha\right)} = n^{-\alpha}
\end{align}

\subsection{Always-go-left}
\label{sec:analysis:alg}
Similarly to Section \ref{sec:analysis:greedy}, where we used symmetric witness trees to upper-bound the probability that a bin contains too many balls when using the \emph{Greedy} scheme, we will use asymmetric witness trees to get an analogous upper bound for the \emph{Always-go-left} scheme. We will make the same simplification as stated in Assumption \ref{assumption:independence}, namely that all the events represented by the edges and nodes of a witness tree are stochastically independent.

\subsubsection{Asymmetric witness trees}
\label{sec:analysis:definitionAsymWT}
With one exception, the definition of an \emph{asymmetric witness tree} is the same as the definition of a symmetric witness tree, including the definition of activation. Only its structure differs from that of a symmetric tree. An asymmetric witness tree has the structure of a $d$-ary Fibonacci tree $T_d\left(k \right)$, which is defined as follows.
\begin{compactitem}
\item $T_d(1)$ is a single node.
\item $T_d(2) = T_d(1)$.
\item For $3\leq k \leq d$, $T_d(k)$ is a tree with $k-1$ children, which are the roots of the trees $T_d(k-1),\ldots,T_d(1)$.
\item For $k>d$, $T_d(k)$ is a tree with $d$ children, which are the roots of the trees $T_d(k-1),\ldots,T_d(k-d)$.
\end{compactitem}
\todo{illustrate}

A $d$-ary asymmetric witness tree of order $L$ has the structure of the Fibonacci tree $T_d(d \cdot L+1)$ and by Equation \ref{eqn:goldenFib} has $F_d(d\cdot L + 1) \geq \Phi_d^{d\cdot L-1}$ leaves. 

For each node we define a label $(h, i)$, where $h, i \in \mathbb{N}$, $0 \leq h \leq L$ and  $1\leq i \leq d$. 
\begin{compactitem}
\item A node that is the root of a Fibonacci tree $T_d(1)$ is assigned the label $(0,1)$.
\item Analogously, a node that is the root of a Fibonacci tree $T_d(2)$ is assigned the label $(0,2)$. Thus, nodes labelled $(0,1)$ or $(0,2)$ do not have children.
\item A node with label $(0, i)$, where $i>2$, has $i-1$ children with labels $(0,i-1)\dots (0,1)$
\item A node with label $(h,i)$ , where $h>0$, has $d$ children with the labels $(h,i-1)\dots(h,1)$,$(h-1,d)\dots(h-1, i)$.
\item The root of a Fibonacci tree $T_d(d\cdot h+i)$ has the label $(h, i)$. 
\end{compactitem}
Thus, our the root node of our witness tree will be labelled $(L, 1)$. Note that the labels are unique within the set of children of a node $v$, but not for all nodes of the tree.

\subsubsection{Construction}
\label{sec:analysis:constructionAsymWT}
The labels defined in the previous section are useful to state the following invariant, which will be maintained while assigning balls to the nodes of the witness tree.

\begin{invariant}
Ball $b_v$ referenced by a node $v$ with label $(h,i)$ was placed in a bin belonging to group $i$. The bin contained at least $h+3$ other balls at the time of insertion of the ball $b_v$.
\end{invariant}

Let $x$ be the bin that contains $L+4$ balls. The root node gets assigned the topmost ball in bin $x$. Each of the $d$ locations of this ball points to a bin with at least $L+3$ balls in it, at the time of its insertion. These are the $d$ bins that were sampled by the Always-go-left scheme and thus bin $i$ belongs to the $i$'th group of bins, where each group consists of $n/d$ bins\footnote{For the sake of simplicity, we assume that $d$ divides $n$ evenly, i.e., that all groups have the exact same size. It can be shown that the analysis still holds if each group is of size $\lfloor\frac{n}{d}\rfloor$ or $\left\lceil\frac{n}{d}\right\rceil$.}. The child node of the root with label $(L-1, i)$ is assigned the topmost ball of the bin the $i$' location points to. The assignment of balls to nodes than proceeds recursively. Given a node $v$ with label $(h, i)$ whose ball $b_v$ was put into some bin belonging to group $i$ and containing at least $l+3$ other balls, we can conclude the following.
\begin{compactitem}
\item For $1\leq j < i$, the $j$'th location of the ball $b_v$ points to a bin containing at least $l+4$ balls at the insertion time of the ball $b_v$. If the $j$'th location contained less balls than the $i$'th location, the Always-go-left scheme would have put the ball $b_v$ into the bin pointed to by location $j$. The topmost ball in location $j$ is assigned to the  child $u$ of $v$ with label $(l,j)$. 
\item For $i < j \leq d$, the $j$'th location contains at least $l+3$ balls. If location $j$ contained less, the Always-go-left scheme would have put the ball $b_v$ into the bin it points to. The topmost ball in the bin that location $j$ points to is assigned to the child node $u$ of $v$ labelled with $(l-1, j)$. \todo{the paper claims that the locations $j$ point to bins with $l+2$ balls. This can not be true, since the topmost, i.e., the $l+2$ ball - call it $b_t$ - gets assigned to a node with label $(l'=l-1, j)$. Since there are $l+1=l'+2$ balls below it, at the time of the placement of ball $b_t$, the bin $j$ it gets put into has only $l'+2$ balls in it, violating the invariant. Same with the case $j<i$.} 
\end{compactitem}

\subsubsection{Probability of activation}
\label{sec:analysis:probabilityAsymWT}
Analogously to the symmetric witness tree, the possible number of asymmetric witness trees is upper-bounded by $w=n^k$ and the probability that all edge events occur is\footnote{As in Section \ref{sec:analysis:greedy}, $k$ denotes the number of nodes of the witness tree and $l$ the number of leaves.} 
\begin{align}
p_e = \left(\frac{d}{n}\right)^{k-1}
\end{align}

The probability that all leaf events occur too is the same as for the symmetric witness tree, but its derivation differs. Again, at any time at most $b'=n/3$ bins contain 3 or more balls, but we do not know how those bins are distributed among the groups. Let $b_i$ be the number of bins in group $i$ that contain 3 or more balls. Then $\beta_i = b_i\cdot d/ n$ is the fraction of the bins $b_i$ in group $i$. Since no more than $n/3$ bins can contain 3 or more balls, the values $b_i$ are subject to the constraint
\begin{align}
\frac{n}{d}\sum_{i=1}^d \beta_i \leq \frac{n}{3}
\end{align}
and thus 
\begin{align}
\sum_{i=1}^d \beta_i \leq \frac{d}{3}
\end{align}

The probability that all $d$ locations of a ball point to a bin containing 3 or more balls is then 
\begin{align}
p_{l_1} = \prod_{i=1}^{d} \beta_i
\end{align}
In other words, a leaf event occurs with probability $p_{l_1}$, which is maximized when all $b_i$ are equal. We thus set $b_i \leq 1/3$ for all $i$ and get as upper bound for the probability that all leaf events occur 
\begin{align}
p_l \leq 3^{-d}
\end{align}

The above results imply the same probability for the activation of an asymmetric witness tree as we derived in Section \ref{sec:analysis:probabilitySymWT} for the activation of a symmetric witness tree, that is
\begin{align}
p_{awt} &\leq n \cdot 2^{-l}
\end{align}
However, the number of leaves is larger, since 
\begin{align}
l \geq \Phi_d^{d\cdot L -1}
\end{align}
holds\footnote{see Section \ref{sec:analysis:definitionAsymWT} for details}.

For any constant $\alpha > 0$, let 
\begin{align}
L &\geq \left\lceil{\frac{\ln\log_2 n + \ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d}}\right\rceil+\frac{1}{d}
\end{align}
\todo{the paper uses $+1$, instead of $+1/d$, but I can't make workout the below result then} and note that 
\begin{align}
L &\geq \frac{\ln\log_2 n}{d\cdot \ln \Phi_d} + \frac{\ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d} + \frac{1}{d}\\
  &\geq \frac{\ln\ln n}{d\cdot \ln \Phi_d} + O\left(1\right)
\end{align}

Then, 
\begin{align}
p_{awt} &\leq n \cdot 2^{-\Phi_d^{d\cdot L -1}} \\
        &= n \cdot 2^{-\Phi_d^{d\cdot \left(\left\lceil{\frac{\ln\log_2 n + \ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d}}\right\rceil+\frac{1}{d}\right) -1}} \\
        &\leq n \cdot 2^{-\Phi_d^{d\cdot \left({\frac{\ln\log_2 n + \ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d}}+\frac{1}{d}\right) -1}} \\
        &= n \cdot 2^{-\Phi_d^{{\frac{\ln\log_2 n + \ln\left(1+\alpha\right)}{d\cdot \ln \Phi_d}}}} \\
        &= n \cdot 2^{-\Phi_d^{\log_{\Phi_d}\log_2 n + \log_{\Phi_d}\left(1+\alpha\right)}} \\
         &= n \cdot 2^{-\Phi_d^{\log_{\Phi_d}\log_2 n}\cdot \Phi_d^{ \log_{\Phi_d}\left(1+\alpha\right)}} \\
         &= n \cdot 2^{-\log_2 n \cdot\left(1+\alpha\right)} \\
         &= n \cdot n ^{-(1+\alpha)} = n^{-\alpha}
\end{align}

\subsection{Non-distinct balls}
\label{sec:analysis:nondistinctBalls}

\subsection{The heavily loaded case}
\label{sec:analysis:moreBalls}



\section{Conclusion}
\label{sec:conclusion}

%bibliography
\bibliographystyle{acm}
\bibliography{../sources} 
\end{document}
